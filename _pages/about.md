---
layout: about
title: about
permalink: /
subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Motto. Etc.

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: # <stuff>

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi,
I'm Luca and I want to make it my purpose to make AI more useful, safe, understandable and controllable and thus make it the most useful tool of humanity, changing the world for the better. Thus I am specifically interested in building more robust, interpretable, aligned and efficient [foundation models](https://en.wikipedia.org/wiki/Foundation_model).

I currently write my Master’s Thesis “Towards Active Learning for Large Language Model Alignment” at Stanford's Institute for Human-Centered AI [(HAI)](https://hai.stanford.edu). My work is about implementing active learning in an online Reinforcement Learning from Human Feedback (RLHF) framework. We reproduce [closed-source results](https://arxiv.org/pdf/2402.00396) made by Google Deepmind, showing that some acquisition functions (e.g. Double Thompson Sampling) can make human preference elicitation way more efficient.

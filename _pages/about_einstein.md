Hi,
I'm Luca and I want to make it my purpose to make AI more useful, safe, understandable and controllable and thus make it the most useful tool of humanity, changing the world for the better. Thus I am specifically interested in building more robust, interpretable, aligned and efficient [foundation models](https://en.wikipedia.org/wiki/Foundation_model).

I currently write my Master’s Thesis “Towards Active Learning for Large Language Model Alignment” at Stanford's Institute for Human-Centered AI [(HAI)](https://hai.stanford.edu). My work is about implementing active learning in an online Reinforcement Learning from Human Feedback (RLHF) framework. We reproduce [closed-source results](https://arxiv.org/pdf/2402.00396) made by Google Deepmind, showing that some acquisition functions (e.g. Double Thompson Sampling) can make human preference elicitation way more efficient.
